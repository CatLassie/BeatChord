{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 Helvetica-Bold;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww22840\viewh13880\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 - CUDA needed to speed up NN performance, to use GPU instead of CPU, probably cause gpu is better at linear algebra (matrix computations)\
- tensors are multidimensional array data structures that are fast on GPUs and have other cool properties that make it easy to work with NNs\
\
\
\
- 1 forward pass calculates all predictions with current weights (parameters)\
- then loss function is calculated (MSE for regression, cross entropy for classification) (real and prediction params are plotted against error to get a surface, we wanna find a small value of error)\
- the gradient of the loss function is calculated with automatic differentiation (autograd).\
\
\
\
- e.g. optimisers like SGD (stochastic gradient descent), where w1 = w1 - learningRate*gradient, or Adam w1 = w1 - (momentumCoef + learningRate*gradient)\
to speed up convergence (Adam = Adaptive Moment Evaluation).\
- optimiser.zero_grad() -> loss.backward() -> optimiser.step(): reset current gradients, compute new ones, adjust params based on new gradients (do this for each iteration)\
\
\
\
- dropout: deactivate random neutrons during training to prevent overfitting (better generalisation)\
\
\
\
- going through once through all training data is -> 1 Epoch\
	- we can feed all data at once (batch gradient descent): parameter adjustment after each epoch\
	- we can feed 1 feature at a time (SGD): param adjustment after each feature (# features adjustments / epoch)\
	- we can feed n (1 < n < All Data) features at a time (mini-batch): param adjustment after each mini batch (const number adjustments / epoch)\
1 epoch can have many batches, gradient adjustment happens after each batch\
NOTE: don\'92t confuse SGD optimiser with SGD batching method, I think its separate stuff\
CODE: TensorDataset can be initialised with your training tensors and passed to loader = DataLoader(tensor, batch_size=100, shuffle=True) to regulate batch size\
and then use a python iterator to get batches and feed them into whatever: iter(loader).next() <- is gonna be 1 batch\
\
\
\
- batch normalisation brings stuff to mean 0 and std. dev. to 1, can be used before or after activation function of a layer (depends on the activation function), maybe\
not suitable for use with dropout layers (BE CAREFUL!)\
- batch norm. allows for faster convergence (nicer, smoother gradients), as a side effect they reduce overfitting which is cool  and also allows to have larger learning rates\
\
\
\
NOTE: switch to model.eval() during prediction (as opposed to model.train()) cause this will deactivate dropout and batch normalisation for example (stuff tags you only want during training)\
NOTE: install the hidden layer whatever library to visualise your NN architecture\
\
\
\
- its cool to visualize features with matplot lib after conv. and pooling layers and put the pretty picture into the thesis (for maybe 1 feature for true/false beat and each chord; the calculated feature not the feature map! maybe also the feature maps (weight matrix) ?)\
- also useful to plot the loss change \
\
\
\
OnsetPeakPickingProcessor for neural nets is easy: low threshold value (approx 0.2), small avg (3-4), and max (1-3) frame number\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1\b \cf0 10-11-2020:\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b0 \cf0 CNN net is a big convolution, you give it a seruqnce, you get a sequence back (any length ?)\
- in beat case, you give it feature with a context of e.g. 513 and instead of putting out 1 label, it gives you a sequence of 513!\
- for prediction the same is true, you should be able to give it 513 or even 2000 at the same time and it gives you back a sequence of beat predictions\
- skipping some feature during training is useful, cause if every feature is used it takes too long + there is a lot of overlap anyway, so a reasonable hopsize can be used\
- using context eats up stuff on the left and right sides and those targets are not used (also skipped ones are not used), but its ok, cause the data is still there for training (cause of context), only the\
targets are missed\
- for beats a context of 8sec should be enough, cause at 60bpm if you have 4 beats per bar and 2 bars would be enough, the its 8 beats -> 8sec (so probably 2**7 layer == 10sec)\
\
\
\
\
SILLY IDEAS:\
- spectrograms are 2D, images have color channels so 3D,\
what if you take the same spectrogram and stack it 3-5-whatever times but shift it every time by a time value (like hop size or window size?)\
to have the temporal data in the third dimension\
\
- since we have a more fine grained input feature shape for beats than for chords, we could feed the beat features into the system and at some point use GAP (global average pooling)\
to average a few features into 1 chord feature ? (not GAP really but like local average pooling :D)\
\
- can GANs be used for something ? (Generative Adversarial Networks)}