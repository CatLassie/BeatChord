{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beat SOTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import madmom\n",
    "from madmom.utils import search_files\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset as Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VARIABLES\n",
    "\n",
    "# random seed\n",
    "SEED = 1\n",
    "\n",
    "# cuda configuration\n",
    "DISABLE_CUDA = False\n",
    "USE_CUDA = not DISABLE_CUDA and torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda:0\" if USE_CUDA else \"cpu\")\n",
    "print(\"CURRENT DEVICE:\", DEVICE)\n",
    "\n",
    "# paths\n",
    "CURRENT_PATH = os.getcwd()\n",
    "\n",
    "MODEL_NAME = 'beat_sota'\n",
    "MODEL_PATH = os.path.join(CURRENT_PATH, 'models')\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)  \n",
    "    \n",
    "FEATURE_EXT = '.feat.npy'\n",
    "FEATURE_PATH = os.path.join(CURRENT_PATH, 'features')\n",
    "ANNOTATION_EXT = '.beats'\n",
    "ANNOTATION_PATH = '../../../datasets/beat_boeck'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND LINE SUPPORT\n",
    "\n",
    "# TODO:\n",
    "\n",
    "VERBOSE = True # args.verbose\n",
    "\n",
    "if VERBOSE:\n",
    "    print('\\n---- EXECUTION STARTED ----\\n')\n",
    "    # print('Command line arguments:\\n\\n', args, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FEATURES\n",
    "\n",
    "feat_paths = search_files(FEATURE_PATH, FEATURE_EXT)\n",
    "anno_paths = search_files(ANNOTATION_PATH, ANNOTATION_EXT)\n",
    "\n",
    "features = [np.load(p) for p in feat_paths]\n",
    "annotations = [madmom.io.load_beats(p) for p in anno_paths]\n",
    "\n",
    "if VERBOSE:\n",
    "    print(len(features), 'feature spectrogram files loaded')\n",
    "    print(len(annotations), 'feature annotation files loaded')\n",
    "  \n",
    "# Conacatenate spectrograms along the time axis\n",
    "# features = np.concatenate(features, axis=1)\n",
    "# print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK PARAMETERS\n",
    "\n",
    "# depth\n",
    "in_size = 1\n",
    "h1_size = 16\n",
    "h2_size = 32\n",
    "\n",
    "# kernel size\n",
    "k_conv_size = 5\n",
    "k_pool_size = 2\n",
    "\n",
    "# fully connected parameters\n",
    "fc_size = 512\n",
    "out_size = 10\n",
    "\n",
    "# number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# learning rate\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES\n",
    "\n",
    "# import data\n",
    "mnist_train = pd.read_csv('../../../datasets/mnist_csv/mnist_train.csv')\n",
    "mnist_test = pd.read_csv('../../../datasets/mnist_csv/mnist_test.csv')\n",
    "\n",
    "# prepare data\n",
    "train = mnist_train.dropna()\n",
    "train_feat = mnist_train.drop('label', axis=1)\n",
    "train_target = mnist_train['label']\n",
    "\n",
    "test = mnist_test.dropna()\n",
    "test_feat = mnist_test.drop('label', axis=1)\n",
    "test_target = mnist_test['label']\n",
    "\n",
    "# convert to tensors\n",
    "train_f = torch.tensor(train_feat.values, dtype=torch.float)\n",
    "train_t = torch.tensor(train_target.values, dtype=torch.long)\n",
    "test_f = torch.tensor(test_feat.values, dtype=torch.float)\n",
    "test_t = torch.tensor(test_target.values, dtype=torch.long)\n",
    "train_f = train_f.reshape(-1,1,28,28)\n",
    "test_f = test_f.reshape(-1,1,28,28)\n",
    "\n",
    "valid_f = train_f[:10000].numpy()\n",
    "valid_t = train_t[:10000].numpy()\n",
    "train_f = train_f[10000:].numpy()\n",
    "train_t = train_t[10000:].numpy()\n",
    "\n",
    "print(train_f.shape)\n",
    "print(train_t.shape)\n",
    "print(valid_f.shape)\n",
    "print(valid_t.shape)\n",
    "print(test_f.shape)\n",
    "print(test_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEAT NETWORK CLASS and DATA SET CLASS for DATA LOADER\n",
    "\n",
    "class BeatNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BeatNet, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Conv2d(in_size, h1_size, k_conv_size),\n",
    "            nn.BatchNorm2d(h1_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = k_pool_size)\n",
    "        )\n",
    "        \n",
    "        self.l2 = nn.Sequential(\n",
    "            nn.Conv2d(h1_size, h2_size, k_conv_size),\n",
    "            nn.BatchNorm2d(h2_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = k_pool_size)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(fc_size, out_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.l1(x)\n",
    "        # print(out.shape)\n",
    "        \n",
    "        out = self.l2(out)\n",
    "        # print(out.shape)\n",
    "        \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        # print(out.shape)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        # print(out.shape)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# Dataset for beats\n",
    "class BeatSet(Dataset):\n",
    "    def __init__(self, feat_list, targ_list):\n",
    "        self.features = feat_list\n",
    "        self.targets = targ_list\n",
    "        self.length = len(self.features)\n",
    "        super(BeatSet, self).__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # get 1 feature/target pair\n",
    "        # convert to PyTorch tensor and return\n",
    "        return torch.from_numpy(self.features[index]), torch.from_numpy(np.asarray(self.targets[index])) #torch.from_numpy(self.targets[index])\n",
    "\n",
    "\n",
    "\n",
    "# helper class for arguments\n",
    "class Args:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / test / predict\n",
    "\n",
    "def train_one_epoch(args, model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Training for one epoch.\n",
    "    \"\"\"\n",
    "    # set model to training mode (activate dropout / batch normalization).\n",
    "    model.train()\n",
    "    t = time.time()\n",
    "    # iterate through all data using the loader\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # move data to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # reset optimizer (clear previous gradients)\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass (calculate output of network for input)\n",
    "        output = model(data.float())\n",
    "        # calculate loss        \n",
    "        loss = F.binary_cross_entropy(output, target)\n",
    "        # do a backward pass (calculate gradients using automatic differentiation and backpropagation)\n",
    "        loss.backward()\n",
    "        # udpate parameters of network using calculated gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print logs\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, took {:.2f}s'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(), time.time()-t))\n",
    "            t = time.time()\n",
    "\n",
    "\n",
    "       \n",
    "def calculate_unseen_loss(model, device, unseen_loader):\n",
    "    \"\"\"\n",
    "    Calculate loss for unseen data (validation or testing)\n",
    "    :return: cumulative loss\n",
    "    \"\"\"\n",
    "    # set model to inference mode (deactivate dropout / batch normalization).\n",
    "    model.eval()\n",
    "    # init cumulative loss\n",
    "    unseen_loss = 0\n",
    "    # no gradient calculation    \n",
    "    with torch.no_grad():\n",
    "        # iterate over test data\n",
    "        for data, target in unseen_loader:\n",
    "            # move data to device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # forward pass (calculate output of network for input)\n",
    "            output = model(data.float())\n",
    "            # claculate loss and add it to our cumulative loss\n",
    "            unseen_loss += F.binary_cross_entropy(output, target, reduction='sum').item() # sum up batch loss\n",
    "\n",
    "    # output results of test run\n",
    "    unseen_loss /= len(unseen_loader.dataset)\n",
    "    print('Average loss: {:.4f}\\n'.format(\n",
    "        unseen_loss, len(unseen_loader.dataset)))\n",
    "\n",
    "    return unseen_loss\n",
    "  \n",
    "    \n",
    "    \n",
    "def predict(model, device, data):\n",
    "    \"\"\"\n",
    "    Predict beat\n",
    "    :return: prediction\n",
    "    \"\"\"\n",
    "    # set model to inference mode (deactivate dropout / batch normalization).\n",
    "    model.eval()\n",
    "    output = None\n",
    "    # move data to device\n",
    "    data = torch.from_numpy(data)\n",
    "    data = data.to(device)\n",
    "    # no gradient calculation\n",
    "    with torch.no_grad():\n",
    "        output = model(data.float())\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    print('Training network...')\n",
    "\n",
    "    # parameters for NN training\n",
    "    args = Args()\n",
    "    args.batch_size = 8 #64\n",
    "    args.max_epochs = 1 #25 #1000\n",
    "    args.patience = 4\n",
    "    args.lr = 0.01 # 0.001, 0.0001\n",
    "    args.momentum = 0.5 #UNUSED\n",
    "    args.log_interval = 100 #100\n",
    "\n",
    "    # setup pytorch\n",
    "    torch.manual_seed(SEED)\n",
    "    \n",
    "    # create model and optimizer\n",
    "    model = BeatNet().to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # setup our datasets for training, evaluation and testing\n",
    "    kwargs = {'num_workers': 4, 'pin_memory': True} if USE_CUDA else {'num_workers': 4}\n",
    "    train_loader = torch.utils.data.DataLoader(BeatSet(train_f, train_t),\n",
    "                                               batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    valid_loader = torch.utils.data.DataLoader(BeatSet(valid_f, valid_t),\n",
    "                                               batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(BeatSet(test_f, test_t),\n",
    "                                              batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    # main training loop\n",
    "    best_validation_loss = 9999\n",
    "    cur_patience = args.patience\n",
    "    for epoch in range(1, args.max_epochs + 1):\n",
    "        # run one epoch of NN training\n",
    "        train_one_epoch(args, model, DEVICE, train_loader, optimizer, epoch)\n",
    "        # validate on validation set\n",
    "        print('\\nValidation Set:')\n",
    "        validation_loss = calculate_unseen_loss(model, DEVICE, valid_loader)\n",
    "        # check for early stopping\n",
    "        if validation_loss < best_validation_loss:\n",
    "            torch.save(model.state_dict(), os.path.join(MODEL_PATH, MODEL_NAME + '.model'))\n",
    "            best_validation_loss = validation_loss\n",
    "            cur_patience = args.patience\n",
    "        else:\n",
    "            # if performance does not improve, we do not stop immediately but wait for 4 iterations (patience)\n",
    "            if cur_patience <= 0:\n",
    "                print('Early stopping, no improvement for %d epochs...' % args.patience)\n",
    "                break\n",
    "            else:\n",
    "                print('No improvement, patience: %d' % cur_patience)\n",
    "                cur_patience -= 1\n",
    "\n",
    "    # testing on test data\n",
    "    print('Evaluate network...')\n",
    "    print('Test Set:')\n",
    "    # calculate loss for test set\n",
    "    calculate_unseen_loss(model, DEVICE, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(test_features): \n",
    "    torch.manual_seed(SEED)\n",
    "    \n",
    "    # load model\n",
    "    model = BeatNet().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(os.path.join(MODEL_PATH, MODEL_NAME + '.model')))\n",
    "    print('model loaded...')\n",
    "    \n",
    "    # calculate actual output for the test data\n",
    "    results_cnn = [None for _ in range(len(test_features))]\n",
    "    # iterate over test tracks\n",
    "    for test_idx, cur_test_feat in enumerate(test_features):\n",
    "        if test_idx % 100 == 0:\n",
    "            completion = int((test_idx / len(test_features))*100)\n",
    "            print(str(completion)+'% complete...')\n",
    "        \n",
    "        # run the inference method\n",
    "        result = predict(model, DEVICE, cur_test_feat)\n",
    "        results_cnn[test_idx] = result.numpy()[0]\n",
    "\n",
    "    return results_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = run_prediction(test_f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
