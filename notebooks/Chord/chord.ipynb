{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import madmom\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset as Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# configurations\n",
    "import scripts.chord_config as cc\n",
    "\n",
    "# feature, target, annotation initializer\n",
    "from scripts.chord_feat import init_data\n",
    "\n",
    "from scripts.chord_util import labels_to_notataion_and_intervals\n",
    "\n",
    "import mir_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VARIABLES\n",
    "\n",
    "# random seed\n",
    "SEED = cc.SEED\n",
    "\n",
    "# cuda configuration\n",
    "USE_CUDA = cc.USE_CUDA\n",
    "DEVICE = cc.DEVICE\n",
    "print(\"CURRENT DEVICE:\", DEVICE)\n",
    "\n",
    "# paths\n",
    "MODEL_NAME = cc.MODEL_NAME\n",
    "MODEL_PATH = cc.MODEL_PATH\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)  \n",
    "    \n",
    "FPS = cc.FPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING PARAMETERS\n",
    "\n",
    "num_epochs = cc.NUM_EPOCHS\n",
    "lr = cc.LR\n",
    "batch_size = cc.BATCH_SIZE\n",
    "patience = cc.PATIENCE\n",
    "\n",
    "feature_context = cc.FEATURE_CONTEXT\n",
    "traininig_hop_size = cc.TRAINING_HOP_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND LINE SUPPORT\n",
    "\n",
    "# TODO:\n",
    "\n",
    "TRAIN = cc.TRAIN\n",
    "PREDICT = cc.PREDICT\n",
    "VERBOSE = cc.VERBOSE\n",
    "\n",
    "if VERBOSE:\n",
    "    print('\\n---- EXECUTION STARTED ----\\n')\n",
    "    # print('Command line arguments:\\n\\n', args, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FEATURES AND ANNOTATIONS, COMPUTE TARGETS\n",
    "train_f, train_t, train_anno, valid_f, valid_t, valid_anno, test_f, test_t, test_anno = init_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK PARAMETERS\n",
    "\n",
    "# CNN\n",
    "\n",
    "# filters\n",
    "cnn_in_size = 1\n",
    "cnn_h1_size = 16 #32\n",
    "cnn_h2_size = 32 #64\n",
    "cnn_h3_size = 64\n",
    "\n",
    "# kernels\n",
    "cnn_k_size = 3\n",
    "cnn_padding = (1,1)\n",
    "cnn_max_pool_k_size = (2,2)\n",
    "\n",
    "cnn_dropout_rate = 0.1\n",
    "\n",
    "# FULLY CONNECTED (by using a 1d convolutional. layer)\n",
    "\n",
    "# filters\n",
    "fc_h1_size = 104 #52 #156 # neurons in FC layers\n",
    "fc_out_size = 13 # 13 outputs for 13 classes\n",
    "\n",
    "# kernels\n",
    "fc_k1_size = (18,11) #(37,22) #(6,22) #22 # something big that would correspond to an FC layer (capture all data into 1 input)\n",
    "fc_k2_size = 1 # second FC layer gets input from first one, filter size is 1\n",
    "\n",
    "# loss function\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "unseen_loss_func = nn.CrossEntropyLoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHORD NETWORK CLASS and DATA SET CLASS for DATA LOADER\n",
    "\n",
    "class ChordNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChordNet, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Conv2d(cnn_in_size, cnn_h1_size, cnn_k_size, padding=cnn_padding),\n",
    "            nn.BatchNorm2d(cnn_h1_size),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size = cnn_max_pool_k_size),\n",
    "            nn.Dropout2d(p = cnn_dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.l2 = nn.Sequential(\n",
    "            nn.Conv2d(cnn_h1_size, cnn_h2_size, cnn_k_size, padding=cnn_padding),\n",
    "            nn.BatchNorm2d(cnn_h2_size),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size = cnn_max_pool_k_size),\n",
    "            nn.Dropout2d(p = cnn_dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.l3 = nn.Sequential(\n",
    "            nn.Conv2d(cnn_h2_size, cnn_h3_size, cnn_k_size, padding=cnn_padding),\n",
    "            nn.BatchNorm2d(cnn_h3_size),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size = cnn_max_pool_k_size),\n",
    "            nn.Dropout2d(p = cnn_dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.lfc1 = nn.Sequential(\n",
    "            nn.Conv2d(cnn_h3_size, fc_h1_size, fc_k1_size), # nn.Conv1d(cnn_h2_size, fc_h1_size, fc_k1_size),\n",
    "            nn.BatchNorm2d(fc_h1_size),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(p = cnn_dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.lfcout = nn.Sequential(\n",
    "            nn.Conv1d(fc_h1_size, fc_out_size, fc_k2_size),\n",
    "            #nn.Softmax(1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        #print(x.shape)\n",
    "\n",
    "        out = self.l1(x)\n",
    "        #print(out.shape)\n",
    "\n",
    "        out = self.l2(out)\n",
    "        #print(out.shape)\n",
    "        \n",
    "        out = self.l3(out)\n",
    "        #print(out.shape)\n",
    "        \n",
    "        #out.squeeze_(2)\n",
    "        #print(out.shape)\n",
    "\n",
    "        out = self.lfc1(out)\n",
    "        #print(out.shape)\n",
    "                \n",
    "        out.squeeze_(2)\n",
    "        #print(out.shape)\n",
    "            \n",
    "        out = self.lfcout(out)\n",
    "        #print(out.shape)\n",
    "        \n",
    "        out.squeeze_(-1)\n",
    "        #print(out.shape)\n",
    "                        \n",
    "        #raise Exception(\"UNDER CONSTRUCTION!\")\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "# Dataset for DataLoader (items are pairs of Context x 81 (time x freq.) spectrogram snippets and 0-1 (0.5) target values)\n",
    "class ChordSet(Dataset):\n",
    "    def __init__(self, feat_list, targ_list, context, hop_size):\n",
    "        self.features = feat_list\n",
    "        self.targets = targ_list\n",
    "        self.context = context\n",
    "        self.side = int((context-1)/2)\n",
    "        self.hop_size = hop_size\n",
    "        # list with snippet count per track\n",
    "        self.snip_cnt = []\n",
    "        # overall snippet count\n",
    "        total_snip_cnt = 0\n",
    "        # calculate overall number of snippets we can get from our data\n",
    "        for feat in feat_list:\n",
    "            cur_len = int(np.ceil((feat.shape[0] - self.side*2) / self.hop_size))\n",
    "            self.snip_cnt.append(cur_len)\n",
    "            total_snip_cnt += cur_len\n",
    "        self.length = int(total_snip_cnt)\n",
    "        super(ChordSet, self).__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # find track which contains snipped with index [index]\n",
    "        overal_pos = 0\n",
    "        for idx, cnt in enumerate(self.snip_cnt):\n",
    "            # check if this track contains the snipped with index [index]\n",
    "            if index < overal_pos+cnt:\n",
    "                break\n",
    "            else:\n",
    "                # if not, add the current tracks snipped count to the overall snipped count already visited\n",
    "                overal_pos += cnt\n",
    "\n",
    "        # calculate the position of the snipped within the track nr. [idx]\n",
    "        position = index-overal_pos\n",
    "        position += self.side\n",
    "\n",
    "        # get snipped and target\n",
    "        sample = self.features[idx][(position-self.side):(position+self.side+1), :]        \n",
    "        target = self.targets[idx][position] # self.targets[idx][position, :]        \n",
    "        # convert to PyTorch tensor and return (unsqueeze is for extra dimension, asarray is cause target is scalar)\n",
    "        return torch.from_numpy(sample).unsqueeze_(0), torch.from_numpy(np.asarray(target))\n",
    "\n",
    "\n",
    "\n",
    "# helper class for arguments\n",
    "class Args:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN / TEST / PREDICT\n",
    "\n",
    "def train_one_epoch(args, model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Training for one epoch.\n",
    "    \"\"\"\n",
    "    # set model to training mode (activate dropout / batch normalization).\n",
    "    model.train()\n",
    "    t = time.time()\n",
    "    # iterate through all data using the loader\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # move data to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # reset optimizer (clear previous gradients)\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass (calculate output of network for input)\n",
    "        output = model(data.float())\n",
    "        # calculate loss        \n",
    "        loss = loss_func(output, target)\n",
    "        # do a backward pass (calculate gradients using automatic differentiation and backpropagation)\n",
    "        loss.backward()\n",
    "        # udpate parameters of network using calculated gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print logs\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, took {:.2f}s'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(), time.time()-t))\n",
    "            t = time.time()\n",
    "\n",
    "        # WORK IN PROGRESS: skip rest of loop\n",
    "        # print('train batch index:', batch_idx)\n",
    "        # break\n",
    "       \n",
    "def calculate_unseen_loss(model, device, unseen_loader):\n",
    "    \"\"\"\n",
    "    Calculate loss for unseen data (validation or testing)\n",
    "    :return: cumulative loss\n",
    "    \"\"\"\n",
    "    # set model to inference mode (deactivate dropout / batch normalization).\n",
    "    model.eval()\n",
    "    # init cumulative loss\n",
    "    unseen_loss = 0\n",
    "    # no gradient calculation    \n",
    "    with torch.no_grad():\n",
    "        # iterate over test data\n",
    "        for data, target in unseen_loader:\n",
    "            # move data to device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # forward pass (calculate output of network for input)\n",
    "            output = model(data.float())\n",
    "            \n",
    "            # WORK IN PROGRESS: skip rest of loop\n",
    "            # continue\n",
    "            \n",
    "            # claculate loss and add it to our cumulative loss\n",
    "            unseen_loss += unseen_loss_func(output, target).item() # sum up batch loss\n",
    "\n",
    "    # output results of test run\n",
    "    unseen_loss /= len(unseen_loader.dataset)\n",
    "    print('Average loss: {:.4f}\\n'.format(\n",
    "        unseen_loss, len(unseen_loader.dataset)))\n",
    "\n",
    "    return unseen_loss\n",
    "  \n",
    "    \n",
    "    \n",
    "def predict(model, device, data, context):\n",
    "    \"\"\"\n",
    "    Predict beat\n",
    "    :return: prediction\n",
    "    \"\"\"\n",
    "    # set model to inference mode (deactivate dropout / batch normalization).\n",
    "    model.eval()\n",
    "    \n",
    "    in_shape = data.shape\n",
    "    side = int((context-1)/2)\n",
    "    outlen = in_shape[0] - 2*side\n",
    "    output = np.empty(outlen)\n",
    "    # move data to device\n",
    "    data = torch.from_numpy(data[None, None, :, :])\n",
    "    data = data.to(device)\n",
    "    # no gradient calculation\n",
    "    with torch.no_grad():\n",
    "        # iterate over input data\n",
    "        for idx in range(outlen):\n",
    "            # calculate output for input data\n",
    "            out_vector = model(data[:, :, idx:(idx+context), :])[0]\n",
    "            _, out_val = torch.max(out_vector.data, 0)            \n",
    "            output[idx] = out_val\n",
    "\n",
    "    # compensate for convolutional context and return output\n",
    "    output = np.append([12 for _ in range(side)], output)\n",
    "    output = np.append(output,[12 for _ in range(side)])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    print('Training network...')\n",
    "\n",
    "    # parameters for NN training\n",
    "    args = Args()\n",
    "    args.batch_size = batch_size #1 #64\n",
    "    args.max_epochs = num_epochs #25 #1000\n",
    "    args.patience = patience #4\n",
    "    args.lr = lr # 0.001, 0.0001\n",
    "    args.momentum = 0.5 #UNUSED\n",
    "    args.log_interval = 100 #100\n",
    "    args.context = feature_context #5\n",
    "    args.hop_size = traininig_hop_size\n",
    "\n",
    "    # setup pytorch\n",
    "    torch.manual_seed(SEED)\n",
    "    \n",
    "    # create model and optimizer\n",
    "    model = ChordNet().to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # setup our datasets for training, evaluation and testing\n",
    "    kwargs = {'num_workers': 4, 'pin_memory': True} if USE_CUDA else {'num_workers': 4}\n",
    "    train_loader = torch.utils.data.DataLoader(ChordSet(train_f, train_t, args.context, args.hop_size),\n",
    "                                               batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    valid_loader = torch.utils.data.DataLoader(ChordSet(valid_f, valid_t, args.context, args.hop_size),\n",
    "                                               batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(ChordSet(test_f, test_t, args.context, args.hop_size),\n",
    "                                              batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    # main training loop\n",
    "    best_validation_loss = 9999\n",
    "    cur_patience = args.patience\n",
    "    for epoch in range(1, args.max_epochs + 1):\n",
    "        # run one epoch of NN training\n",
    "        train_one_epoch(args, model, DEVICE, train_loader, optimizer, epoch)\n",
    "        \n",
    "        # WORK IN PROGRESS: \n",
    "        # return\n",
    "        \n",
    "        # validate on validation set\n",
    "        print('\\nValidation Set:')\n",
    "        validation_loss = calculate_unseen_loss(model, DEVICE, valid_loader)\n",
    "        # check for early stopping\n",
    "        if validation_loss < best_validation_loss:\n",
    "            torch.save(model.state_dict(), os.path.join(MODEL_PATH, MODEL_NAME + '.model'))\n",
    "            best_validation_loss = validation_loss\n",
    "            cur_patience = args.patience\n",
    "        else:\n",
    "            # if performance does not improve, we do not stop immediately but wait for 4 iterations (patience)\n",
    "            if cur_patience <= 0:\n",
    "                print('Early stopping, no improvement for %d epochs...' % args.patience)\n",
    "                break\n",
    "            else:\n",
    "                print('No improvement, patience: %d' % cur_patience)\n",
    "                cur_patience -= 1\n",
    "\n",
    "    # testing on test data\n",
    "    print('Evaluate network...')\n",
    "    print('Test Set:')\n",
    "    # calculate loss for test set\n",
    "    calculate_unseen_loss(model, DEVICE, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(test_features): \n",
    "    args = Args()\n",
    "    args.context = feature_context #5\n",
    "    \n",
    "    torch.manual_seed(SEED)\n",
    "    \n",
    "    # load model\n",
    "    model = ChordNet().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(os.path.join(MODEL_PATH, MODEL_NAME + '.model'))) #, map_location=torch.device('cpu')\n",
    "    print('model loaded...')\n",
    "    \n",
    "    # calculate actual output for the test data\n",
    "    results_cnn = [None for _ in range(len(test_features))]\n",
    "    # iterate over test tracks\n",
    "    for test_idx, cur_test_feat in enumerate(test_features):\n",
    "        if test_idx % 100 == 0:\n",
    "            completion = int((test_idx / len(test_features))*100)\n",
    "            print(str(completion)+'% complete...')\n",
    "        if VERBOSE:\n",
    "            print('file number:', test_idx+1)\n",
    "        \n",
    "        # run the inference method\n",
    "        result = predict(model, DEVICE, cur_test_feat, args.context)\n",
    "        results_cnn[test_idx] = result\n",
    "\n",
    "    return results_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = None\n",
    "\n",
    "if PREDICT:    \n",
    "    # predict chords\n",
    "    if VERBOSE:\n",
    "        print('predicting...')\n",
    "    predicted = run_prediction(test_f)\n",
    "                \n",
    "    # evaluate results\n",
    "    if VERBOSE:\n",
    "        print('evaluating results...')\n",
    "    \n",
    "    p_scores_mic = []\n",
    "    r_scores_mic = []\n",
    "    f1_scores_mic = []\n",
    "    p_scores_w = []\n",
    "    r_scores_w = []\n",
    "    f1_scores_w = []\n",
    "    \n",
    "    weighted_accuracies = []\n",
    "    \n",
    "    for i, pred_chord in enumerate(predicted):        \n",
    "        p_scores_mic.append(precision_score(test_t[i], pred_chord, average='micro'))\n",
    "        r_scores_mic.append(recall_score(test_t[i], pred_chord, average='micro'))\n",
    "        f1_scores_mic.append(f1_score(test_t[i], pred_chord, average='micro'))\n",
    "\n",
    "        p_scores_w.append(precision_score(test_t[i], pred_chord, average='weighted'))\n",
    "        r_scores_w.append(recall_score(test_t[i], pred_chord, average='weighted'))\n",
    "        f1_scores_w.append(f1_score(test_t[i], pred_chord, average='weighted'))\n",
    "        \n",
    "        # mir_eval score (weighted accuracy)\n",
    "\n",
    "        ref_labels, ref_intervals = labels_to_notataion_and_intervals(test_t[i])\n",
    "        est_labels, est_intervals = labels_to_notataion_and_intervals(pred_chord)\n",
    "\n",
    "        est_intervals, est_labels = mir_eval.util.adjust_intervals(\n",
    "            est_intervals, est_labels, ref_intervals.min(),\n",
    "            ref_intervals.max(), mir_eval.chord.NO_CHORD,\n",
    "            mir_eval.chord.NO_CHORD)\n",
    "\n",
    "        # print('label length before merge', len(ref_labels), len(est_labels))\n",
    "        # print('interval length before merge', len(ref_intervals), len(est_intervals))\n",
    "        merged_intervals, ref_labels, est_labels = mir_eval.util.merge_labeled_intervals(ref_intervals, ref_labels, est_intervals, est_labels)\n",
    "        # print('label length after merge', len(ref_labels), len(est_labels))\n",
    "        # print('interval length after merge', len(merged_intervals))\n",
    "\n",
    "        durations = mir_eval.util.intervals_to_durations(merged_intervals)\n",
    "        comparison = mir_eval.chord.root(ref_labels, est_labels)\n",
    "        score = mir_eval.chord.weighted_accuracy(comparison, durations)\n",
    "\n",
    "        weighted_accuracies.append(score)\n",
    "    \n",
    "    print('Precision (micro):', np.mean(p_scores_mic))\n",
    "    print('Recall (mico):', np.mean(r_scores_mic))\n",
    "    print('F-measure (micro):', np.mean(f1_scores_mic))\n",
    "    \n",
    "    print('Precision (weighted):', np.mean(p_scores_w))\n",
    "    print('Recall (weighted):', np.mean(r_scores_w))\n",
    "    print('F-measure (weighted):', np.mean(f1_scores_w))\n",
    "    \n",
    "    print('Weighted accuracies (mir_eval):', np.mean(weighted_accuracies))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
