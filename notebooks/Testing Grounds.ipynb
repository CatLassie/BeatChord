{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Grounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "print(torch.get_default_dtype()) # defautlt is 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.Tensor([[1,2,3],[4,5,6]])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.is_tensor(t))\n",
    "print(torch.numel(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_uninit = torch.Tensor(2,2)\n",
    "t_rand = torch.rand(2,2)\n",
    "t_int = torch.tensor([5,3]).type(torch.IntTensor)\n",
    "t_fill = torch.full((2,5), fill_value=10)\n",
    "t_zero_like_fill = torch.zeros_like(t_fill)\n",
    "t_diag = torch.eye(5)\n",
    "t_nonzero = torch.nonzero(t_diag)\n",
    "print(t_uninit)\n",
    "print(t_rand)\n",
    "print(t_int)\n",
    "print(t_fill)\n",
    "print(t_zero_like_fill)\n",
    "print(t_diag)\n",
    "print(t_nonzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.rand(2,3)\n",
    "print(t1)\n",
    "t1.fill_(10)\n",
    "print(t1)\n",
    "t1.add_(5)\n",
    "print(t1)\n",
    "t1.sqrt_()\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.linspace(start=0.1, end=10.0, steps=15)\n",
    "print(t2)\n",
    "print(torch.chunk(t2, 3, 0))\n",
    "print(torch.cat((torch.chunk(t2,3,0)), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = torch.Tensor([[10,8,30],[40,5,6],[12,2,21]])\n",
    "print(t3)\n",
    "print(t3[0,1])\n",
    "print(t3[1:,1:])\n",
    "print(t3.size())\n",
    "print(t3.view(9)) # doesnt create a new tensor / array! CAREFUL! THEY SHARE MEMORY!\n",
    "# NOTE: probably useful for conv -> fully connected transition\n",
    "# t3.view(9)[0] = 666\n",
    "# print(t3) # ORIGINAL MODIFIED!!!!\n",
    "print(t3.view(-1, 1)) # -1 means any number of rows here (adjust for 1 column thats specified)\n",
    "print(t3.shape) # similar to size() ?\n",
    "t3_unsqueeze = torch.unsqueeze(t3,2)\n",
    "print(t3_unsqueeze)\n",
    "print(t3_unsqueeze.shape)\n",
    "print(torch.transpose(t3,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4 = torch.Tensor([[10,8,30],[40,5,6],[12,2,100]])\n",
    "t4_s, t4_ind = torch.sort(t4)\n",
    "print(t4_ind) # indices from OG tensor in sorted order\n",
    "t5 = torch.Tensor([-1,-2,3])\n",
    "print(torch.abs(t5))\n",
    "t6 = torch.rand(2,3)\n",
    "t7 = torch.rand(2,3)\n",
    "print(t6)\n",
    "print(t7)\n",
    "print(torch.add(t6,t7))\n",
    "print(torch.mul(t6,t7)) # ELEMENTWISE MULTIPLICATION!\n",
    "print(torch.clamp(t6, min=0.3, max=0.7))\n",
    "\n",
    "t8 = torch.Tensor([1,2])\n",
    "t9 = torch.Tensor([10,20])\n",
    "print(torch.dot(t8,t9)) # DOT PRODUCT ! (THIS WORKS ON VECTORS!!! NOT MATRICES)\n",
    "t10 = torch.Tensor([[1,3],[2,4]])\n",
    "t11 = torch.Tensor([[10,30],[40,20]])\n",
    "print(torch.mm(t10,t11)) # THIS IS THE REAL MATRIX MULTIPLICATION!!!\n",
    "print(torch.argmax(t11, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t12 = torch.rand(3,3)\n",
    "print(t12.numpy()) # THESE ALSO SHARE MEMORY!!! CAREFUL!\n",
    "print(type(t12.numpy()))\n",
    "print(type(torch.from_numpy(t12.numpy())))\n",
    "# NOTE torch.as_tensor() copies only when necessary, mostly uses same memory\n",
    "# torch.tensor() always makes a copy!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
