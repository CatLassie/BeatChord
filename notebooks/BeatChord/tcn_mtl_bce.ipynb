{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCN MTL BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import madmom\n",
    "from madmom.features.onsets import OnsetPeakPickingProcessor\n",
    "# from madmom.features.beats import BeatTrackingProcessor\n",
    "from madmom.evaluation.beats import BeatEvaluation\n",
    "from madmom.evaluation.beats import BeatMeanEvaluation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset as Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# configurations\n",
    "import scripts.tcn_mtl_config as tmc\n",
    "\n",
    "# feature, target, annotation initializer\n",
    "from scripts.tcn_mtl_feat import init_data\n",
    "\n",
    "from scripts.chord_util import labels_to_notataion_and_intervals\n",
    "from scripts.chord_util import targets_to_one_hot\n",
    "\n",
    "import mir_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VARIABLES\n",
    "\n",
    "# random seed\n",
    "SEED = tmc.SEED\n",
    "\n",
    "# cuda configuration\n",
    "USE_CUDA = tmc.USE_CUDA\n",
    "DEVICE = tmc.DEVICE\n",
    "print(\"CURRENT DEVICE:\", DEVICE)\n",
    "\n",
    "# paths\n",
    "MODEL_NAME = tmc.MODEL_NAME\n",
    "MODEL_PATH = tmc.MODEL_PATH\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)  \n",
    "    \n",
    "FPS = tmc.FPS\n",
    "\n",
    "# peak picker params\n",
    "THRESHOLD = tmc.THRESHOLD\n",
    "PRE_AVG = tmc.PRE_AVG\n",
    "POST_AVG = tmc.POST_AVG\n",
    "PRE_MAX = tmc.PRE_MAX\n",
    "POST_MAX = tmc.POST_MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING PARAMETERS\n",
    "\n",
    "num_epochs = tmc.NUM_EPOCHS\n",
    "lr = tmc.LR\n",
    "\n",
    "feature_context = tmc.FEATURE_CONTEXT\n",
    "traininig_hop_size = tmc.TRAINING_HOP_SIZE\n",
    "\n",
    "batch_size = tmc.BATCH_SIZE\n",
    "patience = tmc.PATIENCE\n",
    "\n",
    "beat_loss_weight = tmc.BEAT_BCE_LOSS_WEIGHT\n",
    "chord_loss_weight = tmc.CHORD_BCE_LOSS_WEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMAND LINE SUPPORT\n",
    "\n",
    "# TODO:\n",
    "\n",
    "TRAIN = tmc.TRAIN\n",
    "TRAIN_EXISTING = tmc.TRAIN_EXISTING\n",
    "PREDICT = tmc.PREDICT\n",
    "TRAIN_ON_BEAT = tmc.TRAIN_ON_BEAT\n",
    "TRAIN_ON_CHORD = tmc.TRAIN_ON_CHORD\n",
    "VERBOSE = tmc.VERBOSE\n",
    "\n",
    "if VERBOSE:\n",
    "    print('\\n---- EXECUTION STARTED ----\\n')\n",
    "    print('Train:', TRAIN)\n",
    "    print('Train existing model:', TRAIN_EXISTING)\n",
    "    print('Predict', PREDICT)\n",
    "    print('Training on beat data:', TRAIN_ON_BEAT, ', training on chord data:', TRAIN_ON_CHORD)\n",
    "    # print('Command line arguments:\\n\\n', args, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FEATURES AND ANNOTATIONS, COMPUTE TARGETS\n",
    "train_f, train_b_t, train_b_anno, train_c_t, train_c_anno, valid_f, valid_b_t, valid_b_anno, valid_c_t, valid_c_anno, test_f, test_b_t, test_b_anno, test_c_t, test_c_anno = init_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_c_t_1hot = targets_to_one_hot(train_c_t)\n",
    "valid_c_t_1hot = targets_to_one_hot(valid_c_t)\n",
    "test_c_t_1hot = targets_to_one_hot(test_c_t)\n",
    "\n",
    "print('example of 1-hot-encoded target shape:', train_c_t_1hot[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base approach\n",
    "\n",
    "total_labels = 0\n",
    "beat_occurences = 0\n",
    "#class_occurences = np.zeros(14, np.float32)\n",
    "for i, target in enumerate(train_c_t_1hot):\n",
    "    for j, frame in enumerate(target):\n",
    "        total_labels = total_labels + 1\n",
    "        beat_occurences = beat_occurences + train_b_t[i][j]\n",
    "        #for k, label in enumerate(frame):\n",
    "        #    class_occurences[k] = class_occurences[k] + label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd approach\n",
    "'''\n",
    "chord_occurences = 0\n",
    "for i, c in enumerate(class_occurences):\n",
    "    if i < 13:\n",
    "        chord_occurences = chord_occurences + c\n",
    "\n",
    "weight_values = np.full(14, chord_occurences, np.float32)\n",
    "weight_values[13] = class_occurences[13]\n",
    "weight_values = (total_labels - weight_values) / weight_values\n",
    "'''\n",
    "\n",
    "# 1st approach\n",
    "#class_occurences[13] = beat_occurences\n",
    "#weight_values = (total_labels - class_occurences) / class_occurences\n",
    "#weight_values[13] = weight_values[13]*13 # to balance off 13vs1 neurons\n",
    "\n",
    "#print(class_occurences)\n",
    "#print(chord_occurences)\n",
    "#print('loss weights:', weight_values)\n",
    "\n",
    "#print(beat_occurences)\n",
    "#print(total_labels)\n",
    "beat_weight = (total_labels - beat_occurences) / beat_occurences\n",
    "weight_values = [1,1,1,1,1,1,1,1,1,1,1,1,1,beat_weight]\n",
    "print('beat loss weight:', beat_weight)\n",
    "#print('loss weights:', weight_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NETWORK PARAMETERS\n",
    "\n",
    "# CNN\n",
    "\n",
    "LAST_CNN_KERNEL_FREQUENCY_SIZE = tmc.LAST_CNN_KERNEL_FREQUENCY_SIZE\n",
    "\n",
    "# filters\n",
    "cnn_in_size = 1\n",
    "cnn_h1_size = 16\n",
    "cnn_h2_size = 32\n",
    "cnn_h3_size = 64\n",
    "\n",
    "# kernels\n",
    "cnn_k_1_size = 3\n",
    "cnn_k_2_size = (1, 2)\n",
    "cnn_padding = (1,0)\n",
    "cnn_max_pool_k_size = (1,3)\n",
    "\n",
    "cnn_dropout_rate = 0.1\n",
    "\n",
    "# TCN\n",
    "\n",
    "tcn_layer_num = 8 #11\n",
    "\n",
    "# filters\n",
    "tcn_h_size = 64\n",
    "\n",
    "# kernels\n",
    "tcn_k_size = 5\n",
    "tcn_dilations = [2**x for x in range(0, tcn_layer_num)]\n",
    "tcn_paddings = [2*x for x in tcn_dilations]\n",
    "\n",
    "tcn_dropout_rate = 0.1\n",
    "\n",
    "# FULLY CONNECTED (by using a 1d convolutional. layer)\n",
    "\n",
    "# filters\n",
    "fc_h_size = 64\n",
    "fc_out_size = 14\n",
    "\n",
    "# kernels\n",
    "fc_k_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEAT NETWORK CLASS and DATA SET CLASS for DATA LOADER\n",
    "\n",
    "class TCNMTLNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TCNMTLNet, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Conv2d(cnn_in_size, cnn_h1_size, cnn_k_1_size, padding=cnn_padding),\n",
    "            nn.BatchNorm2d(cnn_h1_size),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size = cnn_max_pool_k_size),\n",
    "            nn.Dropout2d(p = cnn_dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.l2 = nn.Sequential(\n",
    "            nn.Conv2d(cnn_h1_size, cnn_h2_size, cnn_k_1_size, padding=cnn_padding),\n",
    "            nn.BatchNorm2d(cnn_h2_size),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size = cnn_max_pool_k_size),\n",
    "            nn.Dropout2d(p = cnn_dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.l2b = nn.Sequential(\n",
    "            nn.Conv2d(cnn_h2_size, cnn_h3_size, cnn_k_1_size, padding=cnn_padding),\n",
    "            nn.BatchNorm2d(cnn_h3_size),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool2d(kernel_size = cnn_max_pool_k_size),\n",
    "            nn.Dropout2d(p = cnn_dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.l3 = nn.Sequential(\n",
    "            nn.Conv2d(cnn_h3_size, cnn_h3_size, cnn_k_2_size),\n",
    "            #nn.BatchNorm2d(cnn_h3_size),\n",
    "            # NOTE: if needed try Instance normalization (InstanceNorm2d)\n",
    "            nn.ELU(),\n",
    "            nn.Dropout2d(p = cnn_dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.ld1 = nn.Sequential(\n",
    "                nn.Conv1d(tcn_h_size, tcn_h_size, tcn_k_size, padding=tcn_paddings[0], dilation=tcn_dilations[0]),\n",
    "                nn.BatchNorm1d(tcn_h_size),\n",
    "                nn.ELU(),\n",
    "                nn.Dropout(p = tcn_dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.ld2 = nn.Sequential(\n",
    "                nn.Conv1d(tcn_h_size, tcn_h_size, tcn_k_size, padding=tcn_paddings[1], dilation=tcn_dilations[1]),\n",
    "                nn.BatchNorm1d(tcn_h_size),\n",
    "                nn.ELU(),\n",
    "                nn.Dropout(p = tcn_dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.ld3 = nn.Sequential(\n",
    "                nn.Conv1d(tcn_h_size, tcn_h_size, tcn_k_size, padding=tcn_paddings[2], dilation=tcn_dilations[2]),\n",
    "                nn.BatchNorm1d(tcn_h_size),\n",
    "                nn.ELU(),\n",
    "                nn.Dropout(p = tcn_dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.ld4 = nn.Sequential(\n",
    "                nn.Conv1d(tcn_h_size, tcn_h_size, tcn_k_size, padding=tcn_paddings[3], dilation=tcn_dilations[3]),\n",
    "                nn.BatchNorm1d(tcn_h_size),\n",
    "                nn.ELU(),\n",
    "                nn.Dropout(p = tcn_dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.ld5 = nn.Sequential(\n",
    "                nn.Conv1d(tcn_h_size, tcn_h_size, tcn_k_size, padding=tcn_paddings[4], dilation=tcn_dilations[4]),\n",
    "                nn.BatchNorm1d(tcn_h_size),\n",
    "                nn.ELU(),\n",
    "                nn.Dropout(p = tcn_dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.ld6 = nn.Sequential(\n",
    "                nn.Conv1d(tcn_h_size, tcn_h_size, tcn_k_size, padding=tcn_paddings[5], dilation=tcn_dilations[5]),\n",
    "                nn.BatchNorm1d(tcn_h_size),\n",
    "                nn.ELU(),\n",
    "                nn.Dropout(p = tcn_dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.ld7 = nn.Sequential(\n",
    "                nn.Conv1d(tcn_h_size, tcn_h_size, tcn_k_size, padding=tcn_paddings[6], dilation=tcn_dilations[6]),\n",
    "                nn.BatchNorm1d(tcn_h_size),\n",
    "                nn.ELU(),\n",
    "                nn.Dropout(p = tcn_dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.ld8 = nn.Sequential(\n",
    "                nn.Conv1d(tcn_h_size, tcn_h_size, tcn_k_size, padding=tcn_paddings[7], dilation=tcn_dilations[7]),\n",
    "                nn.BatchNorm1d(tcn_h_size),\n",
    "                nn.ELU(),\n",
    "                nn.Dropout(p = tcn_dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.lfc = nn.Sequential(\n",
    "            nn.Conv1d(fc_h_size, fc_out_size, fc_k_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # print(x.shape)\n",
    "\n",
    "        out = self.l1(x)\n",
    "        # print(out.shape)\n",
    "\n",
    "        out = self.l2(out)\n",
    "        # print(out.shape)\n",
    "        out = self.l2b(out)\n",
    "\n",
    "        out = self.l3(out)\n",
    "        # print(out.shape)\n",
    "        \n",
    "        out.squeeze_(-1)\n",
    "        # print(out.shape)\n",
    "        \n",
    "        out = self.ld1(out)\n",
    "        out = self.ld2(out)\n",
    "        out = self.ld3(out)\n",
    "        out = self.ld4(out)\n",
    "        out = self.ld5(out)\n",
    "        out = self.ld6(out)\n",
    "        out = self.ld7(out)\n",
    "        out = self.ld8(out)\n",
    "        # print(out.shape)\n",
    "        \n",
    "        out = self.lfc(out)\n",
    "        # print(out.shape)\n",
    "                \n",
    "        #out_beat = out[:, 13:, :]\n",
    "        #out_chord = out[:, :13, :]\n",
    "        \n",
    "        #print(out_beat.shape)\n",
    "        #print(out_chord.shape)\n",
    "        \n",
    "        #out_beat = out_beat.squeeze(1)\n",
    "        #out_chord = out_chord.squeeze(1)\n",
    "        \n",
    "        #print(out_beat.shape)\n",
    "        #print(out_chord.shape)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "# Dataset for DataLoader (items are pairs of Context x 81 (time x freq.) spectrogram snippets and 0-1 (0.5) target values)\n",
    "class TCNMTLSet(Dataset):\n",
    "    def __init__(self, feat_list, targ_b_list, targ_c_list, context, hop_size):\n",
    "        self.features = feat_list\n",
    "        self.b_targets = targ_b_list\n",
    "        self.c_targets = targ_c_list\n",
    "        self.context = context\n",
    "        self.hop_size = hop_size\n",
    " \n",
    "        # list with snippet count per track\n",
    "        self.snip_cnt = []\n",
    "        # overall snippet count\n",
    "        total_snip_cnt = 0\n",
    "        # calculate overall number of snippets we can get from our data\n",
    "        for feat in feat_list:\n",
    "            if feat.shape[0]- self.context >= 0: # !!! WARNING: was > previously !!!\n",
    "                cur_len = int(np.floor((feat.shape[0] - self.context)/hop_size) + 1)\n",
    "                self.snip_cnt.append(cur_len)\n",
    "                total_snip_cnt += cur_len\n",
    "            else:\n",
    "                cur_len = 0\n",
    "                self.snip_cnt.append(cur_len)\n",
    "                total_snip_cnt += cur_len \n",
    "                print(\"warning: skipped 1 example, shape\", feat.shape[0])\n",
    "\n",
    "        self.length = int(total_snip_cnt)\n",
    "        super(TCNMTLSet, self).__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # find track which contains snippet with index [index]\n",
    "        overal_pos = 0\n",
    "        for idx, cnt in enumerate(self.snip_cnt):\n",
    "            # check if this track contains the snippet with index [index]\n",
    "            if index < overal_pos+cnt:\n",
    "                break\n",
    "            else:\n",
    "                # if not, add the current tracks snippet count to the overall snippet count already visited\n",
    "                overal_pos += cnt\n",
    "\n",
    "        # calculate the position of the snippet within the track nr. [idx]\n",
    "        position = index-overal_pos\n",
    "        position *= self.hop_size\n",
    "\n",
    "        # get snippet and target\n",
    "        \n",
    "        sample = self.features[idx][position : position+self.context]\n",
    "        b_target = self.b_targets[idx][position : position+self.context]\n",
    "        c_target = self.c_targets[idx][position : position+self.context]\n",
    "        \n",
    "        # probably will need to be removed when beat 2nd neuron is used\n",
    "        # also b_target will need to be transposed like chord?\n",
    "        b_target_2d = np.expand_dims(b_target, axis=0)\n",
    "\n",
    "        transposed_c_target = np.transpose(np.asarray(c_target))\n",
    "\n",
    "        joint_target = np.concatenate((transposed_c_target, b_target_2d))\n",
    "\n",
    "        # convert to PyTorch tensor and return (unsqueeze is for extra dimension, asarray is cause target is scalar)\n",
    "        return torch.from_numpy(sample).unsqueeze_(0), torch.from_numpy(joint_target)\n",
    "\n",
    "\n",
    "\n",
    "# helper class for arguments\n",
    "class Args:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN / TEST / PREDICT\n",
    "\n",
    "def train_one_epoch(args, model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Training for one epoch.\n",
    "    \"\"\"\n",
    "    # set model to training mode (activate dropout / batch normalization).\n",
    "    model.train()\n",
    "    t = time.time()\n",
    "    # iterate through all data using the loader\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # move data to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # reset optimizer (clear previous gradients)\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass (calculate output of network for input)\n",
    "        output = model(data.float())\n",
    "        #calculate weights\n",
    "        weight = torch.from_numpy(np.array(weight_values, np.float32))\n",
    "        weight = weight.to(device)\n",
    "        weight = weight.unsqueeze(1)\n",
    "        # calculate loss\n",
    "        loss = F.binary_cross_entropy(output, target, weight=weight)                \n",
    "        # do a backward pass (calculate gradients using automatic differentiation and backpropagation)\n",
    "        loss.backward()\n",
    "        # udpate parameters of network using calculated gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print logs\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, took {:.2f}s'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(), time.time()-t))\n",
    "            t = time.time()\n",
    "\n",
    "\n",
    "\n",
    "def calculate_unseen_loss(model, device, unseen_loader):\n",
    "    \"\"\"\n",
    "    Calculate loss for unseen data (validation or testing)\n",
    "    :return: cumulative loss\n",
    "    \"\"\"\n",
    "    # set model to inference mode (deactivate dropout / batch normalization).\n",
    "    model.eval()\n",
    "    # init cumulative loss\n",
    "    unseen_loss = 0\n",
    "    # no gradient calculation    \n",
    "    with torch.no_grad():\n",
    "        # iterate over test data\n",
    "        for data, target in unseen_loader:\n",
    "            # move data to device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # forward pass (calculate output of network for input)\n",
    "            output = model(data.float())\n",
    "            #calculate weights\n",
    "            weight = torch.from_numpy(np.array(weight_values, np.float32))\n",
    "            weight = weight.to(device)\n",
    "            weight = weight.unsqueeze(1)\n",
    "            # claculate loss and add it to our cumulative loss            \n",
    "            unseen_loss += F.binary_cross_entropy(output, target, weight=weight, reduction='sum').item() # sum up batch loss\n",
    "\n",
    "    # output results of test run\n",
    "    unseen_loss /= len(unseen_loader.dataset)\n",
    "    print('Average loss: {:.4f}\\n'.format(\n",
    "        unseen_loss, len(unseen_loader.dataset)))\n",
    "\n",
    "    return unseen_loss\n",
    "  \n",
    "\n",
    "\n",
    "def predict(model, device, data, context):\n",
    "    \"\"\"\n",
    "    Predict beat\n",
    "    :return: prediction\n",
    "    \"\"\"\n",
    "    # set model to inference mode (deactivate dropout / batch normalization).\n",
    "    model.eval()\n",
    "    output = None\n",
    "    # move data to device\n",
    "    data = torch.from_numpy(data[None, None, :, :])\n",
    "    data = data.to(device)\n",
    "    # no gradient calculation\n",
    "    with torch.no_grad():\n",
    "        output = model(data.float())\n",
    "        output_beat = output[:, 13:]\n",
    "        output_chord = output[:, :13]\n",
    "        \n",
    "        _, out_chord_val = torch.max(output_chord.data, 1) # 0 -> batch, 1 -> 13 output neurons, 2 -> data size\n",
    "    return output_beat, out_chord_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    print('Training network...')\n",
    "\n",
    "    # parameters for NN training\n",
    "    args = Args()\n",
    "    args.batch_size = batch_size #1 #64\n",
    "    args.max_epochs = num_epochs #25 #1000\n",
    "    args.patience = patience #4\n",
    "    args.lr = lr # 0.001, 0.0001\n",
    "    args.momentum = 0.5 #UNUSED\n",
    "    args.log_interval = 100 #100\n",
    "    args.context = feature_context #5\n",
    "    args.hop_size = traininig_hop_size\n",
    "\n",
    "    # setup pytorch\n",
    "    torch.manual_seed(SEED)\n",
    "    \n",
    "    # create model and optimizer\n",
    "    model = TCNMTLNet().to(DEVICE)\n",
    "    if TRAIN_EXISTING:\n",
    "        model.load_state_dict(torch.load(os.path.join(MODEL_PATH, MODEL_NAME + '.model')))\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # setup our datasets for training, evaluation and testing\n",
    "    kwargs = {'num_workers': 4, 'pin_memory': True} if USE_CUDA else {'num_workers': 4}\n",
    "    train_loader = torch.utils.data.DataLoader(TCNMTLSet(train_f, train_b_t, train_c_t_1hot, args.context, args.hop_size),\n",
    "                                               batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    valid_loader = torch.utils.data.DataLoader(TCNMTLSet(valid_f, valid_b_t, valid_c_t_1hot, args.context, args.hop_size),\n",
    "                                               batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(TCNMTLSet(test_f, test_b_t, test_c_t_1hot, args.context, args.hop_size),\n",
    "                                              batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    # main training loop\n",
    "    best_validation_loss = 9999999\n",
    "    cur_patience = args.patience\n",
    "    for epoch in range(1, args.max_epochs + 1):\n",
    "        # run one epoch of NN training\n",
    "        train_one_epoch(args, model, DEVICE, train_loader, optimizer, epoch)\n",
    "        \n",
    "        # WORK IN PROGRESS: \n",
    "        # return\n",
    "        \n",
    "        # validate on validation set\n",
    "        print('\\nValidation Set:')\n",
    "        validation_loss = calculate_unseen_loss(model, DEVICE, valid_loader)\n",
    "        # check for early stopping\n",
    "        if validation_loss < best_validation_loss:\n",
    "            torch.save(model.state_dict(), os.path.join(MODEL_PATH, MODEL_NAME + '.model'))\n",
    "            best_validation_loss = validation_loss\n",
    "            cur_patience = args.patience\n",
    "        else:\n",
    "            # if performance does not improve, we do not stop immediately but wait for 4 iterations (patience)\n",
    "            if cur_patience <= 0:\n",
    "                print('Early stopping, no improvement for %d epochs...' % args.patience)\n",
    "                break\n",
    "            else:\n",
    "                print('No improvement, patience: %d' % cur_patience)\n",
    "                cur_patience -= 1\n",
    "\n",
    "    # testing on test data\n",
    "    print('Evaluate network...')\n",
    "    print('Test Set:')\n",
    "    # calculate loss for test set\n",
    "    calculate_unseen_loss(model, DEVICE, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN or TRAIN_EXISTING:\n",
    "    run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(test_features): \n",
    "    args = Args()\n",
    "    args.context = feature_context #5\n",
    "    \n",
    "    torch.manual_seed(SEED)\n",
    "    \n",
    "    # load model\n",
    "    model = TCNMTLNet().to(DEVICE)\n",
    "    model.load_state_dict(torch.load(os.path.join(MODEL_PATH, MODEL_NAME + '.model')))\n",
    "    print('model loaded...')\n",
    "    \n",
    "    # calculate actual output for the test data\n",
    "    b_results_cnn = [None for _ in range(len(test_features))]\n",
    "    c_results_cnn = [None for _ in range(len(test_features))]\n",
    "    # iterate over test tracks\n",
    "    for test_idx, cur_test_feat in enumerate(test_features):\n",
    "        if test_idx % 100 == 0:\n",
    "            completion = int((test_idx / len(test_features))*100)\n",
    "            print(str(completion)+'% complete...')\n",
    "        if VERBOSE:\n",
    "            print('file number:', test_idx+1)\n",
    "        \n",
    "        # run the inference method\n",
    "        b_result, c_result = predict(model, DEVICE, cur_test_feat, args.context)\n",
    "        b_results_cnn[test_idx] = b_result.cpu().numpy()\n",
    "        c_results_cnn[test_idx] = c_result.cpu().numpy()\n",
    "\n",
    "    return b_results_cnn, c_results_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PREDICT:\n",
    "    # predict beats and chords\n",
    "    if VERBOSE:\n",
    "        print('predicting...')\n",
    "    predicted_beats, predicted_chords = run_prediction(test_f) #[test_t[0], test_t[1]]\n",
    "                    \n",
    "    # evaluate results\n",
    "    if VERBOSE:\n",
    "        print('evaluating results...')\n",
    "        \n",
    "    #### CHORDS ################\n",
    "        \n",
    "    chord_p_scores_mic = []\n",
    "    chord_r_scores_mic = []\n",
    "    chord_f1_scores_mic = []\n",
    "    chord_p_scores_w = []\n",
    "    chord_r_scores_w = []\n",
    "    chord_f1_scores_w = []\n",
    "    \n",
    "    chord_weighted_accuracies = []\n",
    "    \n",
    "    for i, pred_chord in enumerate(predicted_chords):        \n",
    "        \n",
    "        pred_chord = pred_chord.squeeze(0) # squeeze cause the dimensions are (1, frame_num, cause of the batch)!!!\n",
    "        \n",
    "        chord_p_scores_mic.append(precision_score(test_c_t[i], pred_chord, average='micro'))\n",
    "        chord_r_scores_mic.append(recall_score(test_c_t[i], pred_chord, average='micro'))\n",
    "        chord_f1_scores_mic.append(f1_score(test_c_t[i], pred_chord, average='micro'))\n",
    "\n",
    "        chord_p_scores_w.append(precision_score(test_c_t[i], pred_chord, average='weighted'))\n",
    "        chord_r_scores_w.append(recall_score(test_c_t[i], pred_chord, average='weighted'))\n",
    "        chord_f1_scores_w.append(f1_score(test_c_t[i], pred_chord, average='weighted'))\n",
    "        \n",
    "        # mir_eval score (weighted accuracy)\n",
    "\n",
    "        ref_labels, ref_intervals = labels_to_notataion_and_intervals(test_c_t[i])\n",
    "        est_labels, est_intervals = labels_to_notataion_and_intervals(pred_chord)\n",
    "\n",
    "        est_intervals, est_labels = mir_eval.util.adjust_intervals(\n",
    "            est_intervals, est_labels, ref_intervals.min(),\n",
    "            ref_intervals.max(), mir_eval.chord.NO_CHORD,\n",
    "            mir_eval.chord.NO_CHORD)\n",
    "\n",
    "        # print('label length before merge', len(ref_labels), len(est_labels))\n",
    "        # print('interval length before merge', len(ref_intervals), len(est_intervals))\n",
    "        merged_intervals, ref_labels, est_labels = mir_eval.util.merge_labeled_intervals(ref_intervals, ref_labels, est_intervals, est_labels)\n",
    "        # print('label length after merge', len(ref_labels), len(est_labels))\n",
    "        # print('interval length after merge', len(merged_intervals))\n",
    "\n",
    "        durations = mir_eval.util.intervals_to_durations(merged_intervals)\n",
    "        comparison = mir_eval.chord.root(ref_labels, est_labels)\n",
    "        score = mir_eval.chord.weighted_accuracy(comparison, durations)\n",
    "\n",
    "        chord_weighted_accuracies.append(score)\n",
    "    \n",
    "    print('\\nCHORD EVALUATION:')\n",
    "    \n",
    "    print('Precision (micro):', np.mean(chord_p_scores_mic))\n",
    "    print('Recall (mico):', np.mean(chord_r_scores_mic))\n",
    "    print('F-measure (micro):', np.mean(chord_f1_scores_mic))\n",
    "    \n",
    "    print('Precision (weighted):', np.mean(chord_p_scores_w))\n",
    "    print('Recall (weighted):', np.mean(chord_r_scores_w))\n",
    "    print('F-measure (weighted):', np.mean(chord_f1_scores_w))\n",
    "    \n",
    "    print('Weighted accuracies (mir_eval):', np.mean(chord_weighted_accuracies))\n",
    "    \n",
    "    #### BEATS ################\n",
    "    \n",
    "    print('\\nBEAT EVALUATION:')\n",
    "    \n",
    "    picked_beats = []\n",
    "    \n",
    "    # beat_picker = BeatTrackingProcessor(fps=FPS) # TODO: replace with OnsetPeakPickingProcessor(fps=FPS)\n",
    "    beat_picker = OnsetPeakPickingProcessor(fps=FPS, threshold=THRESHOLD, pre_avg=PRE_AVG, post_avg=POST_AVG, pre_max=PRE_MAX, post_max=POST_MAX) # TODO: replace with OnsetPeakPickingProcessor(fps=FPS)\n",
    "            \n",
    "    for i, pred_beat in enumerate(predicted_beats):\n",
    "\n",
    "        pred_beat = pred_beat.squeeze(0).squeeze(0)\n",
    "\n",
    "        picked = beat_picker(pred_beat) # squeeze cause the dimensions are (1, frame_num, cause of the batch)!!!\n",
    "        picked_beats.append(picked)\n",
    "                \n",
    "    evals = []\n",
    "    for i, beat in enumerate(picked_beats):\n",
    "        e = madmom.evaluation.beats.BeatEvaluation(beat, test_b_anno[i])\n",
    "        evals.append(e)\n",
    "        \n",
    "    mean_eval = madmom.evaluation.beats.BeatMeanEvaluation(evals)\n",
    "    print(mean_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
